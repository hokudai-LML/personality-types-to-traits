{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "authors = pd.read_csv('author_profiles.csv')\n",
    "authors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_sub = authors.loc[authors['enneagram_type'].notnull()]\n",
    "authors_sub['enneagram_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enne_dict = dict()\n",
    "for ind, row in authors_sub.iterrows():\n",
    "    if row['enneagram_type'] not in enne_dict:\n",
    "        enne_dict[row['enneagram_type']] = 1\n",
    "    else:\n",
    "        enne_dict[row['enneagram_type']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in enne_dict.items():\n",
    "    print('For enneagram type ' + str(k) + ' there are ' + str(v) + ' users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_combo = pd.read_csv('all_authors_combined_liwc.csv')\n",
    "authors_combo = authors_combo.loc[authors_combo['enneagram_type'].notnull()]\n",
    "print(list(authors_combo.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Test for correlation with Enneagram types\n",
    "liwc_cols = ['WC', 'WPS', 'Sixltr', 'Dic', 'funct', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'verb', 'auxverb', 'past', 'present', 'future', 'adverb', 'preps', 'conj', 'negate', 'quant', 'number', 'swear', 'social', 'family', 'friend', 'humans', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'cogmech', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'inhib', 'incl', 'excl', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'relativ', 'motion', 'space', 'time', 'work', 'achieve', 'leisure', 'home', 'money', 'relig', 'death', 'assent', 'nonfl', 'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP']\n",
    "\n",
    "for col in liwc_cols:\n",
    "    results = pearsonr(authors_combo['enneagram_type'], authors_combo[col])\n",
    "    if results[1] < 0.05:\n",
    "        print('Correlation for ' + col + ' is ' + str(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys = comments['subreddit'].unique()\n",
    "comments_dict = dict()\n",
    "unique_users_dict = dict()\n",
    "base_url = 'https://files.pushshift.io/reddit/comments/'\n",
    "files = [\n",
    "    'RC_2015-01.zst',\n",
    "    'RC_2015-02.zst',\n",
    "    'RC_2015-03.zst',\n",
    "    'RC_2015-04.zst',\n",
    "    'RC_2015-05.zst',\n",
    "    'RC_2015-06.zst',\n",
    "    'RC_2015-07.zst',\n",
    "    'RC_2015-08.zst',\n",
    "    'RC_2015-09.zst',\n",
    "    'RC_2015-10.zst',\n",
    "    'RC_2015-11.zst',\n",
    "    'RC_2015-12.zst',\n",
    "    'RC_2016-01.zst',\n",
    "    'RC_2016-02.zst',\n",
    "    'RC_2016-03.zst',\n",
    "    'RC_2016-04.zst',\n",
    "    'RC_2016-05.zst',\n",
    "    'RC_2016-06.zst',\n",
    "    'RC_2016-07.zst',\n",
    "    'RC_2016-08.zst',\n",
    "    'RC_2016-09.zst',\n",
    "    'RC_2016-10.zst',\n",
    "    'RC_2016-11.zst',\n",
    "    'RC_2016-12.zst',\n",
    "    'RC_2017-01.zst',\n",
    "    'RC_2017-02.zst',\n",
    "    'RC_2017-03.zst',\n",
    "    'RC_2017-04.zst',\n",
    "    'RC_2017-05.zst',\n",
    "    'RC_2017-06.zst',\n",
    "    'RC_2017-07.zst',\n",
    "    'RC_2017-08.zst',\n",
    "    'RC_2017-09.zst',\n",
    "    'RC_2017-10.zst',\n",
    "    'RC_2017-11.zst',\n",
    "    'RC_2017-12.zst',\n",
    "    'RC_2018-01.zst',\n",
    "    'RC_2018-02.zst',\n",
    "    'RC_2018-03.zst',\n",
    "    'RC_2018-04.zst',\n",
    "    'RC_2018-05.zst',\n",
    "    'RC_2018-06.zst',\n",
    "    'RC_2018-07.zst',\n",
    "    'RC_2018-08.zst',\n",
    "    'RC_2018-09.zst',\n",
    "    'RC_2018-10.zst',\n",
    "    'RC_2018-11.zst',\n",
    "    'RC_2018-12.zst',\n",
    "    'RC_2019-01.zst',\n",
    "    'RC_2019-02.zst',\n",
    "    'RC_2019-03.zst',\n",
    "    'RC_2019-04.zst',\n",
    "]\n",
    "\n",
    "unique_users_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import ijson\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Batch iterate downlaoded pushshift to conserve memory\n",
    "for file in tqdm(files):\n",
    "    url = 'https://files.pushshift.io/reddit/comments/' + file\n",
    "    subprocess.call(['wget',url,], stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "    subprocess.call(['unzstd',file,'--memory=2048MB'], stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "    with open(file.split('.')[0], 'rb') as f:\n",
    "        for record in ijson.items(f, '', multiple_values=True):\n",
    "            if record['subreddit'] not in comments_dict:\n",
    "                comments_dict[record['subreddit']] = 1\n",
    "                unique_users_dict[record['subreddit']] = set()\n",
    "            else:\n",
    "                comments_dict[record['subreddit']] += 1\n",
    "            unique_users_dict[record['subreddit']].add(record['author'])\n",
    "    dataf = pd.DataFrame({'subreddit': pd.Series(comments_dict.keys()),'comments_count':pd.Series(comments_dict.values()),'unique_authors':pd.Series(len(v) for v in unique_users_dict.values())})\n",
    "    dataf.to_csv('reddit-summary/csv_dump-' + file.split('.')[0] +'.csv', index=False)\n",
    "    for fl in glob.glob('./' + file):\n",
    "        os.remove(fl)\n",
    "    for fl in glob.glob('./' + file.split('.')[0]):\n",
    "        os.remove(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "reddit_path = './reddit-summary/'\n",
    "onlyfiles = [f for f in listdir(reddit_path) if isfile(join(reddit_path, f))]\n",
    "onlyfiles = sorted(onlyfiles)\n",
    "onlyfiles.reverse()\n",
    "new_path = './monthly-report/'\n",
    "previous_name = onlyfiles[0]\n",
    "for csvFile in onlyfiles[1:]:\n",
    "    current = pd.read_csv(reddit_path + csvFile)\n",
    "    previous = pd.read_csv(reddit_path + previous_name)\n",
    "\n",
    "    diff = previous.copy()\n",
    "    diff['comments_count'] = previous['comments_count'] - current['comments_count']\n",
    "    diff['unique_authors'] = previous['unique_authors'] - current['unique_authors']\n",
    "    diff.to_csv(new_path + previous_name, index=False)\n",
    "    previous_name = csvFile\n",
    "\n",
    "# Split collected cumulative data into per-month reports\n",
    "last = pd.read_csv(reddit_path + onlyfiles[-1])\n",
    "last.to_csv(new_path + onlyfiles[-1], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Apply binning to big five traits\n",
    "\n",
    "authors = pd.read_csv('./data/author_profiles.csv')\n",
    "authors['agreeableness_median'] = np.nan\n",
    "authors['conscientiousness_median'] = np.nan\n",
    "authors['openness_median'] = np.nan\n",
    "authors['neuroticism_median'] = np.nan\n",
    "authors['extraversion_median'] = np.nan\n",
    "\n",
    "authors['agreeableness_median'][authors.agreeableness <= authors.agreeableness.median()] = 0\n",
    "authors['agreeableness_median'][authors.agreeableness > authors.agreeableness.median()] = 1\n",
    "\n",
    "authors['conscientiousness_median'][authors.conscientiousness <= authors.agreeableness.median()] = 0\n",
    "authors['conscientiousness_median'][authors.conscientiousness > authors.agreeableness.median()] = 1\n",
    "\n",
    "authors['openness_median'][authors.openness <= authors.agreeableness.median()] = 0\n",
    "authors['openness_median'][authors.openness > authors.agreeableness.median()] = 1\n",
    "\n",
    "authors['neuroticism_median'][authors.neuroticism <= authors.agreeableness.median()] = 0\n",
    "authors['neuroticism_median'][authors.neuroticism > authors.agreeableness.median()] = 1\n",
    "\n",
    "authors['extraversion_median'][authors.extraversion <= authors.agreeableness.median()] = 0\n",
    "authors['extraversion_median'][authors.extraversion > authors.agreeableness.median()] = 1\n",
    "\n",
    "authors['agreeableness_quartile'] = np.nan\n",
    "authors['conscientiousness_quartile'] = np.nan\n",
    "authors['openness_quartile'] = np.nan\n",
    "authors['neuroticism_quartile'] = np.nan\n",
    "authors['extraversion_quartile'] = np.nan\n",
    "\n",
    "authors['agreeableness_quartile'][authors.agreeableness <= authors.agreeableness.quantile(25)] = 0\n",
    "authors['agreeableness_quartile'][(authors.agreeableness > authors.agreeableness.quantile(25)) & (authors.agreeableness < authors.agreeableness.quantile(75))] = 1\n",
    "authors['agreeableness_quartile'][authors.agreeableness >= authors.agreeableness.quantile(75)] = 2\n",
    "\n",
    "authors['conscientiousness_quartile'][authors.conscientiousness <= authors.conscientiousness.quantile(25)] = 0\n",
    "authors['conscientiousness_quartile'][(authors.conscientiousness > authors.conscientiousness.quantile(25)) & (authors.conscientiousness < authors.conscientiousness.quantile(75))] = 1\n",
    "authors['conscientiousness_quartile'][authors.conscientiousness >= authors.conscientiousness.quantile(75)] = 2\n",
    "\n",
    "authors['openness_quartile'][authors.openness <= authors.openness.quantile(25)] = 0\n",
    "authors['openness_quartile'][(authors.openness > authors.openness.quantile(25)) & (authors.openness < authors.openness.quantile(75))] = 1\n",
    "authors['openness_quartile'][authors.openness >= authors.openness.quantile(75)] = 2\n",
    "\n",
    "authors['neuroticism_quartile'][authors.neuroticism <= authors.neuroticism.quantile(25)] = 0\n",
    "authors['neuroticism_quartile'][(authors.neuroticism > authors.neuroticism.quantile(25)) & (authors.neuroticism < authors.neuroticism.quantile(75))] = 1\n",
    "authors['neuroticism_quartile'][authors.neuroticism >= authors.neuroticism.quantile(75)] = 2\n",
    "\n",
    "authors['extraversion_quartile'][authors.extraversion <= authors.extraversion.quantile(25)] = 0\n",
    "authors['extraversion_quartile'][(authors.extraversion > authors.extraversion.quantile(25)) & (authors.extraversion < authors.extraversion.quantile(75))] = 1\n",
    "authors['extraversion_quartile'][authors.extraversion >= authors.extraversion.quantile(75)] = 2\n",
    "\n",
    "authors.to_csv('./data/author_profiles_extended_normal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "# Add new values (previously binned)\n",
    "files = ['./res/agreeableness_median-LR-NP/preds.csv', './res/conscientiousness_median-LR-NP/preds.csv', './res/extraversion_median-LR-NP/preds.csv', './res/neuroticism_median-LR-NP/preds.csv', './res/openness_median-LR-NP/preds.csv']\n",
    "dim = ['agreeableness_median', 'conscientiousness_median', 'extraversion_median', 'neuroticism_median', 'openness_median']\n",
    "data = pd.read_csv('./data/mbti_enne_pred.csv')\n",
    "for file, dimension in zip(files, dim):\n",
    "    preds = pd.read_csv(file)\n",
    "    preds[dimension + '_higher'] = preds.confidence.apply(lambda x: ['0.' + s for s in re.findall(r'\\\\d+', x)][-1])\n",
    "    preds[dimension + '_lower'] = preds.confidence.apply(lambda x: ['0.' + s for s in re.findall(r'\\\\d+', x)][-5])\n",
    "    merger = preds[['author', dimension + '_higher', dimension + '_lower']]\n",
    "    data = pd.merge(data, merger, how='inner', on = 'author')\n",
    "\n",
    "\n",
    "\n",
    "files = ['./res/agreeableness_quartile-LR-NP/preds.csv', './res/conscientiousness_quartile-LR-NP/preds.csv', './res/extraversion_quartile-LR-NP/preds.csv', './res/neuroticism_quartile-LR-NP/preds.csv', './res/openness_quartile-LR-NP/preds.csv']\n",
    "dim = ['agreeableness_quartile', 'conscientiousness_quartile', 'extraversion_quartile', 'neuroticism_quartile', 'openness_quartile']\n",
    "for file, dimension in zip(files, dim):\n",
    "    preds = pd.read_csv(file)\n",
    "    preds[dimension + '_lower'] = preds.confidence.apply(lambda x: ['0.' + s for s in re.findall(r'\\\\d+', x)][-9])\n",
    "    preds[dimension + '_middle'] = preds.confidence.apply(lambda x: ['0.' + s for s in re.findall(r'\\\\d+', x)][-5])\n",
    "    preds[dimension + '_higher'] = preds.confidence.apply(lambda x: ['0.' + s for s in re.findall(r'\\\\d+', x)][-1])\n",
    "    merger = preds[['author', dimension + '_lower', dimension + '_middle', dimension + '_higher']]\n",
    "    data = pd.merge(data, merger, how='inner', on = 'author')\n",
    "    \n",
    "\n",
    "data.to_csv('./data/mbti_enne_pred_extended_normal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
